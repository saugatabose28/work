{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModuleTesting_bert.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAEmyfe6PrlhMcUJLI8508",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saugatabose28/work/blob/main/ModuleTesting_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KuTl1VnNfsX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==3.0.0\n",
        "!pip install emoji\n",
        "!pip install onnx onnxruntime\n",
        "!pip install scikit-plot\n",
        "!pip install plot-metric\n",
        "!pip install pyod\n",
        "!pip install transformers[onnx]\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(1)\n",
        "\n",
        "import random\n",
        "import emoji as emoji\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from numpy import dstack\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy.linalg import norm\n",
        "from transformers import AutoModel\n",
        "from transformers import BertModel, BertTokenizer\n",
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.utils.example import visualize#generate dataset\n",
        "data_path=\"drive/My Drive/Colab Notebooks\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    \n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')        \n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "          sequence_output, pooled_output = self.bert(ids,attention_mask=mask)\n",
        "          return sequence_output"
      ],
      "metadata": {
        "id": "9JQgpwL3Y2sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset():\n",
        "    data = pd.read_csv(f\"{data_path}/preProcessedBalancedSampleDataset.csv\")\n",
        "    return data['tweet'].tolist(), data['class']\n",
        "\n",
        "def data_process(data, labels):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    for sentence in data:\n",
        "        bert_inp = bert_tokenizer.__call__(sentence, max_length=10,\n",
        "                                           padding='max_length', pad_to_max_length=True,\n",
        "                                           truncation=True, return_token_type_ids=False)\n",
        "\n",
        "        input_ids.append(bert_inp['input_ids'])\n",
        "        attention_masks.append(bert_inp['attention_mask'])\n",
        "\n",
        "    input_ids = np.asarray(input_ids)\n",
        "    attention_masks = np.array(attention_masks)\n",
        "    labels = np.array(labels)\n",
        "    d=np.array(data)\n",
        "    return input_ids, attention_masks, labels,d\n",
        "\n",
        "def load_and_process():\n",
        "    data, labels = read_dataset()\n",
        "    num_of_labels = len(labels.unique())\n",
        "    #input_ids, attention_masks, labels = data_process(pre_process_dataset(data), labels)\n",
        "    input_ids, attention_masks, labels,d = data_process(data, labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels,d\n",
        "# function to train the model\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # iterate over batches\n",
        "    total = len(train_dataloader)\n",
        "    \n",
        "    correct=0\n",
        "    print(total)\n",
        "    seqOutput= torch.zeros(0)\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        #print(batch)\n",
        "        step = i+1\n",
        "        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n",
        "        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n",
        "        filledLength = int(100 * step // total)\n",
        "        bar = 'â–ˆ' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}, accuracy={total_accuracy}', end='')\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [b.to(device) for b in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        del batch\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # clear previously calculated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        output= model(sent_id, mask)\n",
        "        #print(output[:, 0, :].size())\n",
        "        #print(output.data)\n",
        "        seqOutput=torch.cat((seqOutput, output[:, 0, :]),0)\n",
        "        #seqOutput.append(output[:, 0, :])\n",
        "        \n",
        "        \n",
        "        #print(output[:, 0, :].size(), labels.size())       \n",
        "        loss = lossFunction(output[:, 0, :],labels)              \n",
        "        #########Custom loss\n",
        "        #loss = OCSVM_loss(weights,biases,output)\n",
        "        #print(tf.math.reduce_mean(loss.item()))\n",
        "        print(\" training loss %f\" %loss.item())\n",
        "        #####################\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss += float(loss.item())              \n",
        "        \n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        #update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / (len(train_dataloader)*batch_size)\n",
        "\n",
        "    accu=100.*correct/total\n",
        "    \n",
        "    # returns the loss and predictions\n",
        "    return avg_loss,seqOutput\n",
        "\n",
        "# Specify the GPU\n",
        "# Setting up the device for GPU usage\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Load Data-set ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "input_ids, attention_masks, labels,text = load_and_process()\n",
        "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ class distribution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "# class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n",
        "# ~~~~~~~~~~ Split train data-set into train, validation and test sets  ~~~~~~~~~~#\n",
        "#train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
        "#                            random_state=2018, test_size=0.2, stratify=labels)\n",
        "\n",
        "#val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "#                         random_state=2018, test_size=0.2, stratify=temp_labels)\n",
        "\n",
        "#text,temp, labels,tempLabels = train_test_split(df, labels, test_size=0.0)\n",
        "\n",
        "#train_text, val_text, train_labels, val_labels = train_test_split(temp_text, temp_labels,#\n",
        "                         #random_state=2018, test_size=0.2, stratify=temp_labels)\n",
        "\n",
        "#with open(data_path, )\n",
        "a=[]\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "#for ids in test_text[\"input_ids\"]:\n",
        "#  a.append(tokenizer.decode(ids))\n",
        "#d=pd.DataFrame(a)\n",
        "#d.to_csv(f\"{data_path}/testHasocBB.csv\")\n",
        "\n",
        "\n",
        "#del temp_text\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_count = len(labels)\n",
        "#test_count = len(test_labels)\n",
        "#val_count = len(val_labels)\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~ Import BERT Model and BERT Tokenizer ~~~~~~~~~~~~~~~~~~~~~#\n",
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "# bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "# Load the BERT tokenizer\n",
        "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Tokenization ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "# for train set\n",
        "train_seq = torch.tensor(df['input_ids'].tolist())\n",
        "train_mask = torch.tensor(df['attention_masks'].tolist())\n",
        "train_y = torch.tensor(labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "#val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
        "#val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
        "#val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "#test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
        "#test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
        "#test_y = torch.tensor(test_labels.tolist())\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Create DataLoaders ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "# define a batch size\n",
        "batch_size=1\n",
        "print(train_seq.size(),train_y.size())\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "print(len(train_data[0][0]))\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "#print(len(train_sampler[0][0]))\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "#print(input_ids.shape)\n",
        "#print(df.shape)\n",
        "#print(train_data.get_shape())\n",
        "#print((test_text))\n",
        "#d=pd.DataFrame(test_text)\n",
        "#d.to_csv(f\"{data_path}/testDavid.csv\")\n",
        "\n",
        "# wrap tensors\n",
        "#val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "#val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "#val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Freeze BERT Parameters ~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "# freeze all the parameters\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "#model=model1\n",
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"bert\"): # choose whatever you like here\n",
        "    param.requires_grad = False\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "#from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# compute the class weights\n",
        "#class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "#print(class_wts)\n",
        "\n",
        "# convert class weights to tensor\n",
        "#weights = torch.tensor(class_wts, dtype=torch.float)\n",
        "#weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "#cross_entropy = nn.NLLLoss(weight=weights)\n",
        "\n",
        "###try \"squared hinge\"###\n",
        "#lossFunction=nn.MultiMarginLoss(p=2,reduction='sum')\n",
        "###\n",
        "###NLLL loss##\n",
        "lossFunction = nn.NLLLoss()\n",
        "###\n",
        "###hinge loss##\n",
        "#lossFunction=nn.HingeEmbeddingLoss();\n",
        "####\n",
        "###CE loss##\n",
        "#lossFunction = nn.CrossEntropyLoss()\n",
        "####\n",
        "\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "#if os.path.isfile(\"/content/drive/MyDrive/saved_weights.pth\") == False:\n",
        "#if os.path.isfile(\"saved_weights.pth\") == False:\n",
        "    # number of training epochs\n",
        "epochs = 1\n",
        "current = 1\n",
        "# for each epoch\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "#global co\n",
        "while current <= epochs:\n",
        "\n",
        "    print(f'\\nEpoch {current} / {epochs}:')\n",
        "\n",
        "    # train model\n",
        "    \n",
        "    train_loss,output= train()\n",
        "    # evaluate model\n",
        "    \n",
        "    #valid_loss= evaluate()\n",
        "\n",
        "    # save the best model\n",
        "    #if valid_loss < best_valid_loss:\n",
        "    #    best_valid_loss = valid_loss\n",
        "        #torch.save(model.state_dict(), 'saved_weights.pth')\n",
        "\n",
        "        # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    #valid_losses.append(valid_loss)\n",
        "    \n",
        "\n",
        "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
        "    #print(f'Validation Loss: {valid_loss:.3f}')\n",
        "\n",
        "    current = current + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "vFs8bcfC6_MD",
        "outputId": "bbbf2da2-a0e8-42d7-c7c6-dd38e28c624b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([64, 10]) torch.Size([64])\n",
            "10\n",
            "\n",
            "Epoch 1 / 1:\n",
            "64\n",
            "Batch 1/64 |â–ˆ>..................................................................................................| 1.56% complete, loss=0.00, accuracy=0 training loss 0.559574\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-a8ff9f713686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-a8ff9f713686>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# backward pass to calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM-L5i8mTMq_",
        "outputId": "aec6dca2-f677-4c0e-f1fc-00651fece8dd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 768])\n"
          ]
        }
      ]
    }
  ]
}